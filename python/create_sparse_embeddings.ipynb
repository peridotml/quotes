{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "54cde76c-30c8-42c6-9d7b-1b4a8380ee66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "import base64\n",
    "import polars as pl\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_id = 'naver/efficient-splade-VI-BT-large-doc'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)  # Add .to(device) if using a specific device\n",
    "\n",
    "def get_sparse_doc(text: str) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Generate a sparse document representation. \n",
    "    \n",
    "    In order to improve search, non-english characters, punctionation, and \n",
    "    single letters are filtered out. I will consider adding them back if the model is fine-tuned on the\n",
    "    quotes dataset.\n",
    "    \"\"\"\n",
    "    # Tokenize texts\n",
    "    tokens = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, add_special_tokens=True)\n",
    "    # Perform inference\n",
    "    output = model(**tokens)\n",
    "    \n",
    "    # Apply log(1+ReLU(x)) to logits and sum across tokens\n",
    "    vec = torch.log(1 + torch.relu(output.logits)).sum(axis=1).squeeze()\n",
    "    \n",
    "    # Extract non-zero elements as sparse representation\n",
    "    cols = vec.nonzero(as_tuple=False).squeeze()\n",
    "    weights = vec[cols].squeeze().detach()\n",
    "    \n",
    "    # Filter tokens based on conditions and round weights\n",
    "    sparse_dict = {idx.item(): round(weight.item(), 2) \n",
    "                   for idx, weight in zip(cols, weights) \n",
    "                   if 1996 <= idx < 29612 and weight > 0.5}\n",
    "    \n",
    "    return sparse_dict\n",
    "\n",
    "def ids_to_tokens(sparse_dict: Dict[int, float]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Convert token IDs in the sparse representation back to token strings.\n",
    "    \"\"\"\n",
    "    # Generate idx to token mapping\n",
    "    idx2token = {idx: token for token, idx in tokenizer.get_vocab().items()}\n",
    "    \n",
    "    return {idx2token[idx]: weight for idx, weight in sparse_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1d2fe55b-4ea3-4325-b40d-9288108a5ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'change': 0.92, 'text': 1.91, 'converted': 0.88, 'translation': 1.5, 'document': 0.78, 'transition': 0.89, 'texts': 0.93, 'sample': 1.77, 'conversion': 1.31, 'samples': 0.84, 'transformed': 1.77, 'transformation': 3.52, 'trans': 0.54, 'convert': 1.27, 'transform': 2.56, 'merge': 0.51, 'sampling': 0.61, 'converting': 1.05, 'transforming': 2.23, 'transformers': 0.97, 'preview': 0.63, 'transforms': 1.69, 'transformations': 2.11, 'transitions': 0.64}\n"
     ]
    }
   ],
   "source": [
    "### Example Usage\n",
    "text = \"This is a sample text for transformation.\"\n",
    "sparse_doc = get_sparse_doc(text)\n",
    "print(ids_to_tokens(sparse_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203abd67-6a06-4caa-8e4e-6b858b610af9",
   "metadata": {},
   "source": [
    "### Preprocess Quotes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da1b1a2e-bc2e-45c3-84d8-3f47f03ee0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def batch_iter_slice(data, bs):\n",
    "    for i in range(0, len(data), bs):\n",
    "        yield data[i:i+bs]\n",
    "        \n",
    "\n",
    "df = pl.read_parquet(\"quotes.parquet\")\n",
    "df = df.filter(df[\"quote\"].map_elements(lambda x: isEnglish(x)))\n",
    "df_sample = df.sample(110000) # max size of free Pinecone tier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee685c15-4100-4300-a18d-de73266ec28b",
   "metadata": {},
   "source": [
    "### Embed Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc515b0-1c4e-4071-bffc-d87952c868f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = df_sample[\"quote\"].to_list()\n",
    "embeddings = []\n",
    "for quote in tqdm(quotes):\n",
    "    embeddings.append(get_sparse_doc(quote))\n",
    "\n",
    "vectors = []\n",
    "# loop through the data and create dictionaries for upserts\n",
    "for row, output in zip(df_sample.head(n=152424).to_dicts(), embeddings):\n",
    "    filtered_output = {k: v for k, v in output.items() if v > 0.5}\n",
    "    \n",
    "    if list(filtered_output.keys()) and row[\"author\"] is not None:\n",
    "        vectors.append({\n",
    "            'id': str(base64.b64encode(f\"1234{row['row_nr']}\".encode()).decode()),\n",
    "            'sparse_values': {'indices': list(filtered_output.keys()), 'values': list(filtered_output.values())},\n",
    "            'values': [0.0000001], \n",
    "            'metadata': {\"author\": row[\"author\"], \"quote\": row[\"quote\"]}\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2711d621-6408-4477-9d86-7a14bf083de2",
   "metadata": {},
   "source": [
    "### Push to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d54a83cb-4224-4d3e-8c54-09edb8c6f776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = pinecone.Index(os.environ[\"PINECONE_API_KEY\"], \n",
    "                       os.environ[\"PINECONE_HOST\"])\n",
    "\n",
    "for batch in batch_iter_slice(vectors, 1000):\n",
    "    index.upsert(batch)\n",
    "    time.sleep(0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee840a0c-ca03-401c-b754-55f442c56ca7",
   "metadata": {},
   "source": [
    "### Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d4075-f026-4151-9663-50e658b7d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST http://127.0.0.1:8000/query \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"text\": \"computers\", \"filter\": 0.01}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
